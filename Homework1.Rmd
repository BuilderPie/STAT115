---
title: "STAT115 Homework 1"
author: "(your name)"
date: "Due Feb 10, 2019"
output: html_document
---

# Part 0: Odyssey

Please fill out the Odyssey survey so we can create an account for you:
[https://goo.gl/forms/ocweFyQ5xXQqkzdi1](https://goo.gl/forms/ocweFyQ5xXQqkzdi1)

# Part I: Introduction to R

## Problem 1: Installation

**Please install the following R/Bioconductor packages. Some are
needed for this assigment, and others are for Homework 2.**

```{r install, eval = FALSE}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
#add by Dian 2019.3.5
options(repos = c(CRAN = "http://cran.rstudio.com"))

BiocManager::install("affy", version = "3.8")
BiocManager::install("affyPLM", version = "3.8")
BiocManager::install("hgu133plus2.db", version = "3.8")
BiocManager::install("limma", version = "3.8")
BiocManager::install("sva", version = "3.8")

install.packages(c("ggplot2", "dplyr", "tidyr", "HistData", "mvtnorm",
                   "reticulate"))
```


```{r libraries, message = FALSE}
# these packages are needed for HW2
# affy and affyPLM are needed to read the microarray data and run RMA
library(affy)
library(affyPLM)
library(hgu133plus2.db) # for annotation
library(limma) # for linear modeling
library(sva) # for batch effect correction. Contains ComBat and sva.
library(ggplot2) # for plotting
library(dplyr) # for data manipulation
library(reticulate) # needed to run python in Rstudio
# these next two are not essential to this course
library(mvtnorm) # need this to simulate data from multivariate normal
library(HistData) # need this for data
```


## Problem 2: Getting help

You can use the `mean()` function to compute the mean of a vector like
so:

```{r mean}
x1 <- c(1:10, 50)
mean(x1)
```

However, this does not work if the vector contains NAs:

```{r mean-na}
x1_na <- c(1:10, 50, NA)
mean(x1_na)
```

**Please use R documentation to find the mean after excluding NA's (hint: `?mean`)**

```{r problem2}
# your code here
mean(x1_na, na.rm = TRUE)
```

# Part II: Data Manipulation

## Problem 3: Basic Selection

In this question, we will practice data manipulation using a dataset
collected by Francis Galton in 1886 on the heights of parents and their
children. This is a very famous dataset, and Galton used it to come up
with regression and correlation.

The data is available as `GaltonFamilies` in the `HistData` package.
Here, we load the data and show the first few rows. To find out more
information about the dataset, use `?GaltonFamilies`.

```{r loadGalton}
data(GaltonFamilies)
head(GaltonFamilies)
```

a. **Please report the height of the 10th child in the dataset.**

```{r problem3a}
# your code here
print(paste('The height of the 10th child height is: ', GaltonFamilies[10, ]$childHeight, sep = ''))
```

b. **What is the breakdown of male and female children in the dataset?**

```{r problem3b}
# your code here
lenMale <- sum(GaltonFamilies$gender == "male")
lenFemale <- sum(GaltonFamilies$gender == "female")
lenAll <- length(GaltonFamilies$gender)
print(paste("The breakdown of male is: ", sprintf("%.2f", lenMale/lenAll*100), "%", sep=""))
print(paste("The breakdown of female is: ", sprintf("%.2f", lenFemale/lenAll*100), "%", sep=""))

```

c. **How many observations are in Galton's dataset? Please answer this
question without consulting the R help.**

```{r problem3c}
# your code here
print(paste("The number of observations in Galton's dataset is: ", dim(GaltonFamilies)[1], sep = ""))
```

d. **What is the mean height for the 1st child in each family?**

```{r problem3d}
# your code here
print(paste("The mean height for the 1st child in each family is: ", sprintf("%.3f", mean(GaltonFamilies$childHeight[GaltonFamilies$childNum == 1])), sep = ""))
```

e. **Create a table showing the mean height for male and female children.**
```{r problem3e}
# your code here
library(knitr)
df <- data.frame("Height" = c(
  sprintf("%0.2f", mean(GaltonFamilies$childHeight[GaltonFamilies$gender == "male"])),
  sprintf("%0.2f",mean(GaltonFamilies$childHeight[GaltonFamilies$gender == "female"]))))
rownames(df) <- c("Male", "Female")
knitr::kable(df, format = "html", caption = "The mean height for male and female children")
```

f. **What was the average number of children each family had?**

```{r problem3f}
# your code here
print(paste("The average number of children each family had was:", sprintf("%.2f", lenAll / length(levels(GaltonFamilies$family))) ))

```

g. **Convert the children's heights from inches to centimeters and store
it in a column called `childHeight_cm` in the `GaltonFamilies` dataset.
Show the first few rows of this dataset.**

```{r problem3g}
# your code here
GaltonFamilies$childHeight_cm <- GaltonFamilies$childHeight * 2.54
head(GaltonFamilies)
```


## Problem 4: Spurious Correlation

```{r gen-data-spurious, cache = TRUE}
# set seed for reproducibility
set.seed(1234)
N <- 25
ngroups <- 100000
sim_data <- data.frame(group = rep(1:ngroups, each = N),
                       X = rnorm(N * ngroups),
                       Y = rnorm(N * ngroups))
```

In the code above, we generate `r ngroups` groups of `r N` observations
each. In each group, we have X and Y, where X and Y are independent
normally distributed data and have 0 correlation.

a. **Find the correlation between X and Y for each group, and display
the highest correlations.**

Hint: since the data is quite large and your code might take a few
moments to run, you can test your code on a subset of the data first
(e.g. you can take the first 100 groups like so):

```{r subset}
sim_data_sub <- sim_data %>% filter(group <= 100)
# max_sub <- max(sapply(1:100, function(x) cor(sim_data_sub[sim_data_sub$group == x, 2], sim_data_sub[sim_data_sub$group == x, 3])))
corr_sub <- rep(0, 100)
for (i in 1:100){
  corr_sub[i] <- cor(sim_data[((i-1)*N+1):(i*N), 2], sim_data[((i-1)*N+1):(i*N), 3])
}
max_sub <- max(corr_sub)
print(paste("The highest correlation amont the first 100 groups is: ", sprintf("%.3f", max_sub)))
```

In general, this is good practice whenever you have a large dataset:
If you are writing new code and it takes a while to run on the whole
dataset, get it to work on a subset first. By running on a subset, you
can iterate faster.

However, please do run your final code on the whole dataset.

```{r cor, cache = TRUE}
# your code here
# corr_all <- (sapply(1:ngroups, function(x) cor(sim_data[sim_data$group == x, 2], sim_data[sim_data$group == x, 3])))
corr_all <- rep(0, ngroups)
for (i in 1:ngroups){
  corr_all[i] <- cor(sim_data[((i-1)*N+1):(i*N), 2], sim_data[((i-1)*N+1):(i*N), 3])
}
max_all <- max(corr_all)
print(paste("The highest correlation amont the first 100 groups is: ", sprintf("%.3f", max_all)))
```

b. **The highest correlation is around 0.8. Can you explain why we see
such a high correlation when X and Y are supposed to be independent and
thus uncorrelated?**

Because the chance that certain groups simulations X and Y might still coincide to have simlar or the opposite trend to each other, when the iteration time is extremely large. 

# Part III: Plotting

## Problem 5

**Show a plot of the data for the group that had the highest correlation
you found in Problem 4.**

```{r problem5}
# your code here
index <- which(corr_all == max_all)
df2 <- data.frame(Group = rep("Fit", N),
                  X = sim_data[((index-1)*N+1):(index*N), 2], 
                  Y = sim_data[((index-1)*N+1):(index*N), 3])

model <- lm(Y ~ X, data=df2)
df2$FIT <- fitted(model)

p <- ggplot(df2, aes(x=X, y=Y)) +
  theme_classic() +
  scale_color_manual(values=c( "#E69F00")) +
  geom_point() +
  geom_line(data = df2, aes(x = X, y = FIT, colour = Group), size = 1, linetype = "dashed")

p +
  ylim(-2.5, 2.5) + xlim(-2.5, 2.5) +
  labs(title = bquote(atop("Y ~ X , group = " ~ .(index) ~", correlation coefficient = "  ~ .(sprintf("%.3f", max_all))))) + 
  labs(x = bquote("X"),y = bquote("Y"))
```

## Problem 6

We generate some sample data below. The data is numeric, and has 3
columns: X, Y, Z.

```{r gen-data-corr}
N <- 100
Sigma <- matrix(c(1, 0.75, 0.75, 1), nrow = 2, ncol = 2) * 1.5
means <- list(c(11, 3), c(9, 5), c(7, 7), c(5, 9), c(3, 11))
dat <- lapply(means, function(mu)
  rmvnorm(N, mu, Sigma))
dat <- as.data.frame(Reduce(rbind, dat)) %>%
  mutate(Z = as.character(rep(seq_along(means), each = N)))
names(dat) <- c("X", "Y", "Z")
```

a. **Compute the overall correlation between X and Y.**

```{r problem6a}
# your code here
print(paste("The overall correlation between x and y is: ",
      sprintf("%.3f", cor(dat$X, dat$Y)), sep = ""))
```

b. **Make a plot showing the relationship between X and Y. Comment on
the correlation that you see.**

```{r problem6b}
# your code here
model <- lm(Y ~ X, data=dat)
dat$FIT <- fitted(model)

p <- ggplot(dat, aes(x=X, y=Y, group = Z)) +
  theme_classic() +
  # scale_color_manual(values=c( "#E69F00")) +
  geom_point()
  # geom_line(data = dat, aes(x = X, y = FIT, color = "red"), size = 1, linetype = "dashed")

p +
  labs(title = bquote(atop("Y ~ X, correlation coefficient = "  ~ .(sprintf("%.3f", cor(dat$X, dat$Y)))))) + 
  labs(x = bquote("X"),y = bquote("Y"))
```

It can be observed a negative association between the overall X and Y in dat. The correlation coefficient is -0.736, with P-value smaller that 2.2e-16. Five clusters could be found in the plot, whose center of gravity follow a step down tendency. 

c. **Compute the correlations between X and Y for each level of Z.**

```{r problem6c}
# your code here
lm2val <- function(x, y){
  model <- (cor.test(x, y))
  return(c(model$estimate, model$p.value))
}

result <- list()

for (i in 1:5){
  result <- rbind(result, lm2val(dat[((i-1)*N+1):(i*N), 1], dat[((i-1)*N+1):(i*N), 2]))
  print(paste("Level-", i, ": Correlation coefficient is ", sprintf("%.3f", result[i, 1]), ", P-Val is ", sprintf("%.1e", result[i,2]), sep = ""))
}
```

[1] "Level-1: Correlation coefficient is 0.743, P-Val is 8.6e-19"
[1] "Level-2: Correlation coefficient is 0.756, P-Val is 9.3e-20"
[1] "Level-3: Correlation coefficient is 0.772, P-Val is 5.4e-21"
[1] "Level-4: Correlation coefficient is 0.785, P-Val is 4.5e-22"
[1] "Level-5: Correlation coefficient is 0.730, P-Val is 7.1e-18"

d. **Make a plot showing the relationship between X and Y, but this
time, color the points using the value of Z. Comment on the result,
especially any differences between this plot and the previous plot.**

```{r problem6d}
# your code here
model <- lm(Y ~ X, data=dat)
dat$FIT <- fitted(model)

p <- ggplot(dat, aes(x=X, y=Y, group = Z)) +
  theme_classic() +
  # scale_color_manual(values=c( "#E69F00")) +
  geom_point(aes(color = Z, shape = Z))
  # geom_line(data = dat, aes(x = X, y = FIT, color = "red"), size = 1, linetype = "dashed")

p +
  labs(title = bquote(atop("Y ~ X, correlation coefficient = "  ~ .(sprintf("%.3f", cor(dat$X, dat$Y)))))) + 
  labs(x = bquote("X"),y = bquote("Y"))
```

Five distinct clusters could be found after colouring points with different Z level. Compared to the previous plot, the current one reveals the inter-relationship among the clusters in a more intuitive manner.  

# Part IV: Microarray Normalization

In this part, we are going to analyze a microarray gene expression
dataset from the following paper using the methods learned from the
lecture:

Xu et al, Science 2012. EZH2 oncogenic activity in castration-resistant prostate cancer cells is Polycomb-independent. PMID: [23239736](http://www.ncbi.nlm.nih.gov/pubmed/23239736)

The expression data is available at [GEO under
GSE39461](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE39461),
and for this HW we only need the first 12 samples. There are two
prostate cancer cell lines used: LNCaP and ABL (please ignore the b DHTb 
and b VEHb  labels). To see the function of EZH2 gene, the authors knocked
down EZH2 (siEZH2) in the two cell lines and examined the genes that are
differentially expressed compared to the control, and generated 3
replicates for each condition. They are also interested in finding
whether the genes regulated by EZH2 are similar or different in the
LNCaP and ABL cell lines.

## Problem 7

**Download the needed CEL files (GSM969458 to GSM969469) to your cwd.
Note your cwd needs to be the same as where your CEL files are, or you
can specify the file names using the argument filenames in ReadAffy.
Load the data in R. Draw pairwise MA plot of the raw probe values for
the 3 ABL in control samples. Do the raw data need normalization?**

```{r MAplot}
# your code here
dirList <- dir(("./data"))
print(dirList)
celFiles <- list.celfiles(path = "data", full.names=TRUE)
data.affy <- ReadAffy(filenames = celFiles)

MAplot(data.affy, pairs = TRUE, which=c(1,3,5),
       plot.method = "smoothScatter", cex = 0.9)
```
My answer is yes, because these three control groups have different median value. Without normalization, it would be bias the downstream analysis.

## Problem 8

**Use RMA, which includes background correction, quantile normalization,
and expression index estimation, to obtain the expression level of each
gene. This will generate an expression matrix, where genes are in rows
and samples are in columns. What are the assumptions behind RMA quantile
normalization? Draw a pairwise MA plot of the expression index for the 3
ABL control samples after RMA. Is the RMA normalization successful?**

```{r rma}
# your code here
data.rma <- rma(data.affy)
expr.rma <- exprs(data.rma) # format as table
head(expr.rma[, c(1,3,5)])
MAplot(data.rma, pairs = TRUE, which=c(1,3,5),
       plot.method = "smoothScatter", cex = 0.9)
```
The assumptions behind RMA quantile normalization is most genes / probes don't change between two conditions.

My answer is yes. First the median value for three normalized control groups all reach to zero. Second the shape of three red lines are very close to each other.

# Part V: Python

## Problem 9

Given a list of finite integer numbers: e.g. -2, 1, 7, -4, 5, 2, -3, -6, 4, 3, -8, -1, 6, -7, -9, -5,
Write a python script to maximize the Z where Z is the sum of the
numbers from location X to location Y on this list. Be aware, your
algorithm should look at each number ONLY ONCE from left to right.

Hint: You can use dynamic programming to solve this problem with <20
lines of codes.

```{python dynamic-programming}
arr = [-2, 1, 7, -4, 5, 2, -3, -6, 4, 3, -8, -1, 6, -7, -9, -5]
# your code here


```

```{python}
def solution(A):
    N= len(A)
    maxSlice = 0
    maxSoFar = 0
    
    for i in range(0, N):
        maxSoFar = max(A[i], A[i] + maxSoFar)
        if maxSoFar > maxSlice:
            maxSlice = maxSoFar
    return maxSlice

arr = [-2, 1, 7, -4, 5, 2, -3, -6, 4, 3, -8, -1, 6, -7, -9, -5]
print(solution(arr))
```

